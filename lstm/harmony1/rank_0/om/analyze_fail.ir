# 1.This file shows the parsed IR info when graph evaluating failed to help find the problem.
# 2.You can search the last `------------------------>` to the node which is inferred failed.
# 3.Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.ir to get more instructions.
# ===============================================================================

subgraph attr:
subgraph instance: after_grad.21 : 0x13a082218
# In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:583/                    def after_grad(*args):/
subgraph @after_grad.21(%para1_args0, %para2_args1, %para3_sequential.0.weight_ih_l0, %para4_sequential.0.weight_hh_l0, %para5_sequential.0.bias_ih_l0, %para6_sequential.0.bias_hh_l0, %para7_sequential.1.weight, %para8_sequential.1.bias, %para9_sequential.3.weight, %para10_sequential.3.bias, %para11_sequential.5.weight, %para12_sequential.5.bias) {
  %1([CNode]33) = MakeTuple(%para1_args0, %para2_args1)
      : (<Tensor[Float32], (32, 130, 6)>, <Tensor[Int32], (32)>) -> (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 130, 6), (32))>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:583/                    def after_grad(*args):/
  %2(34) = UnpackGraph(call @forward_fn.3, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 130, 6), (32))>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
  %3([CNode]35) = MakeTuple(%para3_sequential.0.weight_ih_l0, %para4_sequential.0.weight_hh_l0, %para5_sequential.0.bias_ih_l0, %para6_sequential.0.bias_hh_l0, %para7_sequential.1.weight, %para8_sequential.1.bias, %para9_sequential.3.weight, %para10_sequential.3.bias, %para11_sequential.5.weight, %para12_sequential.5.bias)
      : (<Ref[Tensor[Float32]], (128, 6)>, <Ref[Tensor[Float32]], (128, 32)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (32, 32)>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (32, 32)>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (2, 32)>, <Ref[Tensor[Float32]], (2)>) -> (<Tuple[Ref[Tensor[Float32]]*10], TupleShape((128, 6), (128, 32), (128), (128), (32, 32), (32), (32, 32), (32), (2, 32), (2))>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
  %4(34) = S-Prim-grad(%2, %3)
      : (<Func, NoShape>, <Tuple[Ref[Tensor[Float32]]*10], TupleShape((128, 6), (128, 32), (128), (128), (32, 32), (32), (32, 32), (32), (2, 32), (2))>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/

#------------------------> 0
  %5(34) = UnpackCall-unpack_call(%4, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 130, 6), (32))>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
  Return(%5)
      : (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
}
# Order:
#   1: @after_grad.21:34{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> forward_fn.3, [2]: [CNode]33}
#   2: @after_grad.21:34{[0]: ValueNode<DoSignaturePrimitive> S-Prim-grad, [1]: 34, [2]: [CNode]35}
#   3: @after_grad.21:34{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.36, [1]: 34, [2]: [CNode]33}
#   4: @after_grad.21:[CNode]37{[0]: ValueNode<Primitive> Return, [1]: 34}


subgraph attr:
core : 1
subgraph instance: UnpackCall.22 : 0x13a1ba818
# In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
subgraph @UnpackCall.22(%para13_, %para14_) {
  %1(34) = TupleGetItem(%para14_24, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 130, 6), (32))>, <Int64, NoShape>) -> (<Tensor[Float32], (32, 130, 6)>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
  %2(34) = TupleGetItem(%para14_24, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((32, 130, 6), (32))>, <Int64, NoShape>) -> (<Tensor[Int32], (32)>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/

#------------------------> 1
  %3(34) = %para13_23(%1, %2)
      : (<Tensor[Float32], (32, 130, 6)>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
}
# Order:
#   1: @UnpackCall.22:34{[0]: 23, [1]: 34, [2]: 34}
#   2: @UnpackCall.22:34{[0]: ValueNode<Primitive> Return, [1]: 34}


subgraph attr:
k_graph : 1
core : 1
subgraph instance: forward_fn.25 : 0x13a1e9418
# In file /Users/sinnus/WorkSpace/Lightweight-Transformer-Models-For-HAR-on-Mobile-Devices/lstm/harmony1/good.py:95/    def forward_fn(data, label):/
subgraph @forward_fn.25 parent: [subgraph @forward_fn.38](%para15_, %para16_) {
  %1(34) = $(forward_fn.38):J[side_effect_propagate: I64(1)](%para-1_39)
      : (<Func, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/

#------------------------> 2
  %2(34) = %1(%para15_forward_fn, %para16_forward_fn)
      : (<Tensor[Float32], (32, 130, 6)>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
  %3(34) = TupleGetItem(%2, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
  %4(34) = TupleGetItem(%2, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
  %5(34) = HyperMapPy-hyper_map[ones_like_leaf]{fn_leaf=MultitypeFuncGraph-ones_like_leaf{(TypeType), (Tensor), (Number), (COOTensor), (CSRTensor), (Func)}}(%3)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
  %6(34) = %4(%5)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
  %7(34) = TupleGetItem(%6, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
  %8(34) = Partial[side_effect_propagate: I64(1)](MultitypeFuncGraph-env_get{(EnvType, Tensor), (EnvType, MapTensor)}, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
  %9(34) = HyperMap-hyper_map(%8, %para-1_40)
      : (<null>, <Tuple[Ref[Tensor[Float32]]*10], TupleShape((128, 6), (128, 32), (128), (128), (32, 32), (32), (32, 32), (32), (2, 32), (2))>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
  %10(34) = MakeTuple(%3, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
  Return(%10)
      : (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
}
# Order:
#   1: @forward_fn.25:34{[0]: 34, [1]: forward_fn, [2]: forward_fn}
#   2: @forward_fn.25:34{[0]: ValueNode<Primitive> TupleGetItem, [1]: 34, [2]: ValueNode<Int64Imm> 0}
#   3: @forward_fn.25:34{[0]: ValueNode<Primitive> TupleGetItem, [1]: 34, [2]: ValueNode<Int64Imm> 1}
#   4: @forward_fn.25:34{[0]: ValueNode<HyperMapPy> MetaFuncGraph-hyper_map[ones_like_leaf].41, [1]: 34}
#   5: @forward_fn.25:34{[0]: 34, [1]: 34}
#   6: @forward_fn.25:34{[0]: ValueNode<Primitive> TupleGetItem, [1]: 34, [2]: ValueNode<Int64Imm> 0}
#   7: @forward_fn.25:34{[0]: ValueNode<Primitive> Partial, [1]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-env_get.42, [2]: 34}
#   8: @forward_fn.25:34{[0]: ValueNode<HyperMap> MetaFuncGraph-hyper_map.43, [1]: 34, [2]: 40}
#   9: @forward_fn.25:34{[0]: ValueNode<Primitive> MakeTuple, [1]: 34, [2]: 34}
#  10: @forward_fn.25:34{[0]: ValueNode<Primitive> Return, [1]: 34}


subgraph attr:
defer_inline : 1
subgraph instance: forward_fn.3 : 0x13a0b1418
# In file /Users/sinnus/WorkSpace/Lightweight-Transformer-Models-For-HAR-on-Mobile-Devices/lstm/harmony1/good.py:95/    def forward_fn(data, label):/
subgraph @forward_fn.3 parent: [subgraph @after_grad.21](%para17_data, %para18_label) {

#------------------------> 3
  %1(logits) = call @__main___Network_construct.26(%para17_data)
      : (<Tensor[Float32], (32, 130, 6)>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/WorkSpace/Lightweight-Transformer-Models-For-HAR-on-Mobile-Devices/lstm/harmony1/good.py:96/        logits = model(data)/
  %2(loss) = call @mindspore_nn_loss_loss_CrossEntropyLoss_construct.44(%1, %para18_label)
      : (<null>, <Tensor[Int32], (32)>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/WorkSpace/Lightweight-Transformer-Models-For-HAR-on-Mobile-Devices/lstm/harmony1/good.py:97/        loss = loss_fn(logits, label)/
  %3([CNode]45) = S-Prim-MakeTuple(%2, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/WorkSpace/Lightweight-Transformer-Models-For-HAR-on-Mobile-Devices/lstm/harmony1/good.py:98/        return loss, logits/
  %4([CNode]46) = GradAux-aux_fn(%3, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/ops/composite/base.py:584/                        return grad_(fn, weights)(*args)/
  Return(%4)
      : (<null>)
      #scope: (Default)
      # In file /Users/sinnus/WorkSpace/Lightweight-Transformer-Models-For-HAR-on-Mobile-Devices/lstm/harmony1/good.py:98/        return loss, logits/
}
# Order:
#   1: @forward_fn.3:logits{[0]: ValueNode<FuncGraph> __main___Network_construct.26, [1]: data}
#   2: @forward_fn.3:loss{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_CrossEntropyLoss_construct.44, [1]: logits, [2]: label}
#   3: @forward_fn.3:[CNode]45{[0]: ValueNode<DoSignaturePrimitive> S-Prim-MakeTuple, [1]: loss, [2]: logits}
#   4: @forward_fn.3:[CNode]47{[0]: ValueNode<Primitive> Return, [1]: [CNode]46}
#   5: @forward_fn.3:[CNode]46{[0]: ValueNode<GradAux> MetaFuncGraph-aux_fn.48, [1]: [CNode]45, [2]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
subgraph instance: __main___Network_construct.26 : 0x13a06f418
# In file /Users/sinnus/WorkSpace/Lightweight-Transformer-Models-For-HAR-on-Mobile-Devices/lstm/harmony1/good.py:60/    def construct(self, x):/
subgraph @__main___Network_construct.26 parent: [subgraph @after_grad.21](%para19_x) {

#------------------------> 4
  %1(logits) = call @mindspore_nn_layer_container_SequentialCell_construct.27(%para19_x)
      : (<Tensor[Float32], (32, 130, 6)>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/WorkSpace/Lightweight-Transformer-Models-For-HAR-on-Mobile-Devices/lstm/harmony1/good.py:61/        logits = self.sequential(x)/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /Users/sinnus/WorkSpace/Lightweight-Transformer-Models-For-HAR-on-Mobile-Devices/lstm/harmony1/good.py:62/        return logits/
}
# Order:
#   1: @__main___Network_construct.26:logits{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct.27, [1]: x}
#   2: @__main___Network_construct.26:[CNode]49{[0]: ValueNode<Primitive> Return, [1]: logits}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct.27 : 0x13a06fa18
# In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:279/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct.27 parent: [subgraph @after_grad.21](%para20_input_data) {

#------------------------> 5
  %1([CNode]50) = call @↵mindspore_nn_layer_container_SequentialCell_construct.28(I64(0), %para20_input_data)
      : (<Int64, NoShape>, <Tensor[Float32], (32, 130, 6)>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct.27:[CNode]51{[0]: ValueNode<FuncGraph> ms_len_with_iterable_check.52, [1]: [CNode]53}
#   2: @mindspore_nn_layer_container_SequentialCell_construct.27:[CNode]50{[0]: ValueNode<FuncGraph> ↵mindspore_nn_layer_container_SequentialCell_construct.28, [1]: ValueNode<Int64Imm> 0, [2]: input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct.27:[CNode]54{[0]: ValueNode<Primitive> Return, [1]: [CNode]50}


subgraph attr:
training : 1
subgraph instance: ↵mindspore_nn_layer_container_SequentialCell_construct.28 : 0x13a1f7418
# In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
subgraph @↵mindspore_nn_layer_container_SequentialCell_construct.28 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct.27](%para21_, %para22_) {
  %1([CNode]53) = $(mindspore_nn_layer_container_SequentialCell_construct.27):MakeTuple(call @mindspore_nn_layer_rnns_LSTM_construct.55, call @mindspore_nn_layer_basic_Dense_construct.31, call @mindspore_nn_layer_activation_ReLU_construct.56, call @mindspore_nn_layer_basic_Dense_construct.57, call @mindspore_nn_layer_activation_ReLU_construct.58, call @mindspore_nn_layer_basic_Dense_construct.59)
      : (<Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>) -> (<Tuple[Func*6], TupleShape(NoShape, NoShape, NoShape, NoShape, NoShape, NoShape)>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
  %2([CNode]51) = $(mindspore_nn_layer_container_SequentialCell_construct.27):call @ms_len_with_iterable_check.52(%1)
      : (<Tuple[Func*6], TupleShape(NoShape, NoShape, NoShape, NoShape, NoShape, NoShape)>) -> (<Int64, NoShape>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
  %3([CNode]60) = MultitypeFuncGraph-less{(String, String), (Number, Tensor), (Tensor, Tensor), (Tensor, Number), (Number, Number), (Tuple, Tuple), (List, List)}(%para21_@[CNode]29, %2)
      : (<Int64, NoShape>, <Int64, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
  %4([CNode]61) = Switch(%3, call @↻mindspore_nn_layer_container_SequentialCell_construct.30, call @↓mindspore_nn_layer_container_SequentialCell_construct.62)
      : (<Bool, NoShape>, <Func, NoShape>, <Func, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/

#------------------------> 8
  %5([CNode]63) = %4()
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
}
# Order:
#   1: @↵mindspore_nn_layer_container_SequentialCell_construct.28:[CNode]60{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.9, [1]: @[CNode]29, [2]: [CNode]51}
#   2: @↵mindspore_nn_layer_container_SequentialCell_construct.28:[CNode]61{[0]: ValueNode<Primitive> Switch, [1]: [CNode]60, [2]: ValueNode<FuncGraph> ↻mindspore_nn_layer_container_SequentialCell_construct.30, [3]: ValueNode<FuncGraph> ↓mindspore_nn_layer_container_SequentialCell_construct.62}
#   3: @↵mindspore_nn_layer_container_SequentialCell_construct.28:[CNode]63{[0]: [CNode]61}
#   4: @↵mindspore_nn_layer_container_SequentialCell_construct.28:[CNode]64{[0]: ValueNode<Primitive> Return, [1]: [CNode]63}


subgraph attr:
training : 1
subgraph instance: ↻mindspore_nn_layer_container_SequentialCell_construct.30 : 0x13a1f8018
# In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
subgraph @↻mindspore_nn_layer_container_SequentialCell_construct.30 parent: [subgraph @↵mindspore_nn_layer_container_SequentialCell_construct.28]() {
  %1([CNode]29) = MultitypeFuncGraph-add{(Number, Tensor), (Number, Number), (Tuple, Tuple), (Tensor, List), (Tuple, Tensor), (Tensor, Tuple), (Tensor, Number), (COOTensor, COOTensor), (List, Tensor), (List, List), (Tensor, Tensor), (String, String), (COOTensor, Tensor), (RowTensor, Tensor), (NoneType, NoneType), (CSRTensor, CSRTensor), (Tensor, COOTensor)}(%para21_@[CNode]29, I64(1))
      : (<Int64, NoShape>, <Int64, NoShape>) -> (<Int64, NoShape>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
  %2([CNode]65) = StopGradient(%1)
      : (<Int64, NoShape>) -> (<Int64, NoShape>)
      #scope: (Default)
      # In file /Users/sinnus/WorkSpace/Lightweight-Transformer-Models-For-HAR-on-Mobile-Devices/lstm/harmony1/good.py:61/        logits = self.sequential(x)/
  %3([CNode]53) = $(mindspore_nn_layer_container_SequentialCell_construct.27):MakeTuple(call @mindspore_nn_layer_rnns_LSTM_construct.55, call @mindspore_nn_layer_basic_Dense_construct.31, call @mindspore_nn_layer_activation_ReLU_construct.56, call @mindspore_nn_layer_basic_Dense_construct.57, call @mindspore_nn_layer_activation_ReLU_construct.58, call @mindspore_nn_layer_basic_Dense_construct.59)
      : (<Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Func, NoShape>) -> (<Tuple[Func*6], TupleShape(NoShape, NoShape, NoShape, NoShape, NoShape, NoShape)>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
  %4([CNode]66) = call @ms_iter.20(%3)
      : (<Tuple[Func*6], TupleShape(NoShape, NoShape, NoShape, NoShape, NoShape, NoShape)>) -> (<Tuple[Func*6], TupleShape(NoShape, NoShape, NoShape, NoShape, NoShape, NoShape)>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
  %5(cell) = S-Prim-getitem(%4, %para21_@[CNode]29)
      : (<Tuple[Func*6], TupleShape(NoShape, NoShape, NoShape, NoShape, NoShape, NoShape)>, <Int64, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/

#------------------------> 9
  %6(input_data) = %5(%para22_фinput_data)
      : (<Tensor[Float32], (32, 130, 6)>) -> (<Tuple[Tensor[Float32],Tuple[Tensor[Float32]*2]], TupleShape((32, 130, 32), TupleShape((1, 32, 32), (1, 32, 32)))>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:281/            input_data = cell(input_data)/

#------------------------> 7
  %7([CNode]67) = call @↵mindspore_nn_layer_container_SequentialCell_construct.28(%1, %6)
      : (<Int64, NoShape>, <Tuple[Tensor[Float32],Tuple[Tensor[Float32]*2]], TupleShape((32, 130, 32), TupleShape((1, 32, 32), (1, 32, 32)))>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
  %8([CNode]68) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <Int64, NoShape>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
}
# Order:
#   1: @↻mindspore_nn_layer_container_SequentialCell_construct.30:[CNode]66{[0]: ValueNode<FuncGraph> ms_iter.20, [1]: [CNode]53}
#   2: @↻mindspore_nn_layer_container_SequentialCell_construct.30:cell{[0]: ValueNode<DoSignaturePrimitive> S-Prim-getitem, [1]: [CNode]66, [2]: @[CNode]29}
#   3: @↻mindspore_nn_layer_container_SequentialCell_construct.30:[CNode]29{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.12, [1]: @[CNode]29, [2]: ValueNode<Int64Imm> 1}
#   4: @↻mindspore_nn_layer_container_SequentialCell_construct.30:input_data{[0]: cell, [1]: фinput_data}
#   5: @↻mindspore_nn_layer_container_SequentialCell_construct.30:[CNode]67{[0]: ValueNode<FuncGraph> ↵mindspore_nn_layer_container_SequentialCell_construct.28, [1]: [CNode]29, [2]: input_data}
#   6: @↻mindspore_nn_layer_container_SequentialCell_construct.30:[CNode]69{[0]: ValueNode<Primitive> Return, [1]: [CNode]68}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct.31 : 0x13a1a3e18
# In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:620/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct.31 parent: [subgraph @after_grad.21](%para23_x) {

#------------------------> 10
  %1([CNode]70) = call @L-mindspore_nn_layer_basic_Dense_construct.32(%para23_x, %para8_sequential.1.bias, %para7_sequential.1.weight)
      : (<Tuple[Tensor[Float32],Tuple[Tensor[Float32]*2]], TupleShape((32, 130, 32), TupleShape((1, 32, 32), (1, 32, 32)))>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (32, 32)>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct.31:[CNode]70{[0]: ValueNode<FuncGraph> L-mindspore_nn_layer_basic_Dense_construct.32, [1]: x, [2]: sequential.1.bias, [3]: sequential.1.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct.31:[CNode]71{[0]: ValueNode<Primitive> Return, [1]: [CNode]70}


subgraph attr:
training : 1
subgraph instance: L-mindspore_nn_layer_basic_Dense_construct.32 : 0x13a13da18
# In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:620/    def construct(self, x):/
subgraph @L-mindspore_nn_layer_basic_Dense_construct.32(%para24_x, %para25_, %para26_) {

#------------------------> 11
  %1(x_shape) = S-Prim-Shape(%para24_x)
      : (<Tuple[Tensor[Float32],Tuple[Tensor[Float32]*2]], TupleShape((32, 130, 32), TupleShape((1, 32, 32), (1, 32, 32)))>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:621/        x_shape = self.shape_op(x)/
  %2([CNode]72) = S-Prim-check_dense_input_shape[constexpr_prim: Bool(1)](%1, "Dense")
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:622/        check_dense_input_shape(x_shape, self.cls_name)/
  %3([CNode]73) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
  %4([CNode]74) = S-Prim-inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        if len(x_shape) != 2:/
  %5([CNode]75) = S-Prim-not_equal(%4, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        if len(x_shape) != 2:/
  %6([CNode]76) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        if len(x_shape) != 2:/
  %7([CNode]77) = Switch(%6, call @L-✓mindspore_nn_layer_basic_Dense_construct.78, call @L-✗mindspore_nn_layer_basic_Dense_construct.79)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        if len(x_shape) != 2:/
  %8([CNode]80) = %7()
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        if len(x_shape) != 2:/
  %9([CNode]82) = call @L-↓mindspore_nn_layer_basic_Dense_construct.81(%8)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
  %10([CNode]83) = Depend[side_effect_propagate: I64(1)](%9, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/container.py:280/        for cell in self.cell_list:/
  Return(%10)
      : (<null>)
      #scope: (Default)
      # In file /Users/sinnus/miniforge3/envs/har/lib/python3.9/site-packages/mindspore/nn/layer/basic.py:623/        if len(x_shape) != 2:/
}
# Order:
#   1: @L-mindspore_nn_layer_basic_Dense_construct.32:x_shape{[0]: ValueNode<DoSignaturePrimitive> S-Prim-Shape, [1]: x}
#   2: @L-mindspore_nn_layer_basic_Dense_construct.32:[CNode]72{[0]: ValueNode<DoSignaturePrimitive> S-Prim-check_dense_input_shape, [1]: x_shape, [2]: ValueNode<StringImm> Dense}
#   3: @L-mindspore_nn_layer_basic_Dense_construct.32:[CNode]74{[0]: ValueNode<DoSignaturePrimitive> S-Prim-inner_len, [1]: x_shape}
#   4: @L-mindspore_nn_layer_basic_Dense_construct.32:[CNode]75{[0]: ValueNode<DoSignaturePrimitive> S-Prim-not_equal, [1]: [CNode]74, [2]: ValueNode<Int64Imm> 2}
#   5: @L-mindspore_nn_layer_basic_Dense_construct.32:[CNode]76{[0]: ValueNode<Primitive> Cond, [1]: [CNode]75, [2]: ValueNode<BoolImm> false}
#   6: @L-mindspore_nn_layer_basic_Dense_construct.32:[CNode]77{[0]: ValueNode<Primitive> Switch, [1]: [CNode]76, [2]: ValueNode<FuncGraph> L-✓mindspore_nn_layer_basic_Dense_construct.78, [3]: ValueNode<FuncGraph> L-✗mindspore_nn_layer_basic_Dense_construct.79}
#   7: @L-mindspore_nn_layer_basic_Dense_construct.32:[CNode]80{[0]: [CNode]77}
#   8: @L-mindspore_nn_layer_basic_Dense_construct.32:[CNode]82{[0]: ValueNode<FuncGraph> L-↓mindspore_nn_layer_basic_Dense_construct.81, [1]: [CNode]80}
#   9: @L-mindspore_nn_layer_basic_Dense_construct.32:[CNode]83{[0]: ValueNode<Primitive> Depend, [1]: [CNode]82, [2]: [CNode]73}
#  10: @L-mindspore_nn_layer_basic_Dense_construct.32:[CNode]71{[0]: ValueNode<Primitive> Return, [1]: [CNode]83}


#===============================================================================
# num of function graphs in stack: 10/13 (Ignored 3 internal frames).
